{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048be8e",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 \n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eaa4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Edge(r\"C:\\Users\\Lenovo\\msedgedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2489da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Edge(\"msedgedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ed6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8258a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Analyst\")\n",
    "\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "location.send_keys(\"Bengaluru\")\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[6]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6151d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "T=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in T[0:10]:\n",
    "    T=i.text\n",
    "    job_title.append(T)\n",
    "    \n",
    "C=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in C[0:10]:\n",
    "    C=i.text\n",
    "    company_name.append(C)\n",
    "    \n",
    "L=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in L[0:10]:\n",
    "    L=i.text\n",
    "    job_location.append(L)\n",
    "\n",
    "E=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in E[0:10]:\n",
    "    E=i.text\n",
    "    experience_required.append(E)\n",
    "    \n",
    "df = pd.DataFrame({' job_title ':job_title,' Job_location ':job_location,' company_name ':company_name,' experience_required ':experience_required})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aabe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2b443",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "location=[]\n",
    "name=[]\n",
    "requirement=[]\n",
    "\n",
    "driver = webdriver.Edge(r\"C:\\Users\\Lenovo\\msedgedriver.exe\")\n",
    "driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "\n",
    "position=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "position.send_keys(\"Data Scientist\")\n",
    "\n",
    "workplace=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input\")\n",
    "workplace.send_keys(\"Bengaluru\")\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[6]\")\n",
    "search.click()\n",
    "\n",
    "\n",
    "P=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in P[0:10]:\n",
    "    P=i.text\n",
    "    title.append(P)\n",
    "    \n",
    "T=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in T[0:10]:\n",
    "    T=i.text\n",
    "    location.append(T)\n",
    "    \n",
    "N=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in N[0:10]:\n",
    "    N=i.text\n",
    "    name.append(N)\n",
    "    \n",
    "R=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in R[0:10]:\n",
    "    R=i.text\n",
    "    requirement.append(R)\n",
    "    \n",
    "    \n",
    "my_df = pd.DataFrame({'title':title,'location':location,'name':name,'requirement':requirement})\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ca302",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required. \n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "\n",
    "driver = webdriver.Edge(r\"C:\\Users\\Lenovo\\msedgedriver.exe\")\n",
    "driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "position=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "position.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicked the search button\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[6]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the location filter by checking the respective box\n",
    "loc_fil=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/section[1]/div[2]/div[5]/div[2]/div[3]/label/i\")\n",
    "loc_fil.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6159e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the salary filter by checking the respective box\n",
    "sa=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/i\")\n",
    "sa.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "ti=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in ti[0:10]:\n",
    "    ti=i.text\n",
    "    job_title.append(ti)\n",
    "    \n",
    "c=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in c[0:10]:\n",
    "    c=i.text\n",
    "    company_name.append(c)\n",
    "    \n",
    "l=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in l[0:10]:\n",
    "    l=i.text\n",
    "    job_location.append(l)\n",
    "    \n",
    "er=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in er[0:10]:\n",
    "    er=i.text\n",
    "    experience_required.append(er)\n",
    "\n",
    "    \n",
    "# dataframe of the scraped data.\n",
    "\n",
    "ds_df = pd.DataFrame({'job_title':job_title,'company_name':company_name,'job_location':job_location,'experience_required':experience_required})\n",
    "ds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa5359",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "\n",
    "\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and \n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the \n",
    "required data as usual\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then \n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa70a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=[]\n",
    "Brand_description=[]\n",
    "Discount=[]\n",
    "price=[]\n",
    "\n",
    "# Flipkart webpage by url : https://www.flipkart.com/\n",
    "\n",
    "driver = webdriver.Edge(r\"C:\\Users\\Lenovo\\msedgedriver.exe\")\n",
    "driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833725ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter “sunglasses” in the search field where “search for products, brands and more” is written\n",
    "find=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "find.send_keys(\"Sunglasses\")\n",
    "time.sleep(5)\n",
    "\n",
    "#click the search icon\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa63c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping brand name on first page\n",
    "b=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in b[0:100]:\n",
    "    b=i.text\n",
    "    Brand.append(b)\n",
    "    \n",
    "    \n",
    "# Scrapping brand description name on first page    \n",
    "    \n",
    "bd=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in bd[0:100]:\n",
    "    bd=i.text\n",
    "    Brand_description.append(bd)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Scrapping discount% on first page\n",
    "\n",
    "w=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "for i in w[0:100]:\n",
    "    w=i.text\n",
    "    Discount.append(w)\n",
    "\n",
    "    \n",
    "# Scrapping price on first page\n",
    "\n",
    "p=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "for i in p[0:100]:\n",
    "    p=i.text\n",
    "    price.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9661357",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_df= pd.DataFrame({\"Brand\":Brand,\"Brand_description\":Brand_description,\"Discount\":Discount,\"price\":price})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ff1a9",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field . \n",
    "3. Then click the search button\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating=[]\n",
    "Review_summary=[]\n",
    "Full_review=[]\n",
    "\n",
    "# Flipkart webpage by url : https://www.flipkart.com/\n",
    "\n",
    "driver = webdriver.Edge(r\"C:\\Users\\Lenovo\\msedgedriver.exe\")\n",
    "driver = webdriver.Edge(\"msedgedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "find=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "find.send_keys(\"iphone11\")\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK\"]')\n",
    "for i in R[0:100]:\n",
    "    R=i.text\n",
    "    Rating.append(R)\n",
    "    \n",
    "    \n",
    "RS=driver.find_element(By.XPATH,'//div[@class=\"_3LWZlK\"]')\n",
    "for i in R[0:100]:\n",
    "    RS=i.text\n",
    "    Rating.append(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d348e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c85acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
